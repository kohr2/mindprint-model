# PPO Training Pipeline Configuration
# Mac Studio M3 Ultra (256GB unified memory, 80 GPU cores)
# Three-phase training: SFT → Reward Model → PPO

# Model
model:
  name: google/gemma-3-12b-it
  dtype: bfloat16  # bfloat16 is more stable than float16, M3 supports it
  device: mps

# SFT settings - conservative for stability
sft:
  epochs_per_topic: 3
  learning_rate: 0.0001  # Standard LoRA learning rate
  batch_size: 4  # Conservative batch size
  gradient_accumulation_steps: 4
  lora_rank: 8
  lora_alpha: 16
  target_modules:
    - q_proj
    - v_proj
    - o_proj

# Reward model settings - improved for stable convergence
reward_model:
  epochs: 3  # Increased from 1 for better convergence
  learning_rate: 0.00001
  batch_size: 8  # Increased from 4 to reduce variance
  use_voice_penalty: true

  # Validation and early stopping
  validation_split: 0.2  # 80/20 train/val split
  early_stopping_patience: 3  # Stop if no improvement for N epochs
  early_stopping_delta: 0.01  # Minimum improvement threshold

  # Learning rate scheduling
  use_lr_scheduler: true
  scheduler_type: cosine  # cosine, linear, or constant
  warmup_ratio: 0.1  # 10% warmup steps

# PPO settings
ppo:
  steps_per_topic: 100
  learning_rate: 0.00001
  batch_size: 4
  ppo_epochs: 4
  kl_penalty: 0.2
  kl_target: 0.05
  adaptive_kl: true
  clip_range: 0.2
  lora_rank: 8
  lora_alpha: 16
  target_modules:
    - q_proj
    - v_proj
    - o_proj

# Thresholds
thresholds:
  topic_pass_threshold: 0.85  # Reward score to pass

# Pipeline control
pipeline:
  merge_after_unit: true
  max_retries_per_topic: 2

# Paths
paths:
  data_dir: ./data/bob_loukas
  output_dir: ./output
  checkpoint_dir: ./checkpoints

# Logging
logging:
  level: INFO
  log_file: ./logs/training.log
